{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#! pip install langchain_community tiktoken langchain-openai langchainhub chromadb langchain youtube-transcript-api pytube"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['LANGCHAIN_TRACING_V2'] = 'true'\n",
    "os.environ['LANGCHAIN_ENDPOINT'] = 'https://api.smith.langchain.com'\n",
    "os.environ['LANGCHAIN_API_KEY'] = \"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\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "#Setting Environment variable\n",
    "os.environ[\"AZURE_OPENAI_API_VERSION\"] = \"2023-07-01-preview\"\n",
    "os.environ[\"AZURE_OPENAI_ENDPOINT\"] = 'https://dskumar.openai.azure.com/'\n",
    "os.environ[\"AZURE_OPENAI_API_KEY\"] =\"62855d6dd08945819bf83aee0c104127\"\n",
    "os.environ[\"AZURE_OPENAI_CHAT_DEPLOYMENT_NAME\"] =\"DskumarDeployment\"\n",
    "os.environ['OPENAI_TYPE']=\"Azure\"\n",
    "os.environ[\"LLM_MODEL\"] = \"gpt-35-turbo-16k\"\n",
    "os.environ[\"LLM_EMBEDDING_MODEL\"] = \"dskumar-text-embedding-ada-002\"\n",
    "\n",
    "from langchain_core.messages import AIMessage,SystemMessage,HumanMessage\n",
    "from langchain_openai import AzureChatOpenAI\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import AzureOpenAIEmbeddings ##embedding type\n",
    "embeddings = AzureOpenAIEmbeddings(model = os.environ[\"LLM_EMBEDDING_MODEL\"],)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022 ANNUAL REPORT\n",
      "Citi is proud of its \n",
      "colleagues in Ukraine, \n",
      "who are supporting  \n",
      "the country through  \n",
      "a devastating war.\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.document_loaders.pdf import PyPDFLoader\n",
    "loader = PyPDFLoader(r\".\\citi-2022-annual-report.pdf\")\n",
    "output=loader.load()\n",
    "print(output[0].page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_openai import AzureOpenAIEmbeddings\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "#loader = WebBaseLoader(\"https://docs.smith.langchain.com/overview\")\n",
    "docs = loader.load()\n",
    "documents = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=1000, chunk_overlap=200\n",
    ").split_documents(docs)\n",
    "vector = FAISS.from_documents(documents, embeddings,\n",
    ")\n",
    "tenk_retriever = vector.as_retriever()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logical Routing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\JANU\\AppData\\Roaming\\Python\\Python311\\site-packages\\langchain_core\\_api\\beta_decorator.py:87: LangChainBetaWarning: The function `with_structured_output` is in beta. It is actively being worked on, so the API may change.\n",
      "  warn_beta(\n"
     ]
    }
   ],
   "source": [
    "from typing import Literal\n",
    "\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.pydantic_v1 import BaseModel, Field\n",
    "\n",
    "# Data model\n",
    "class RouteQuery(BaseModel):\n",
    "    \"\"\"Route a user query to the most relevant datasource.\"\"\"\n",
    "\n",
    "    datasource: Literal[\"tenk_docs\", \"Process_docs\"] = Field(\n",
    "        ...,\n",
    "        description=\"Given a user question choose which datasource would be most relevant for answering their question\",\n",
    "    )\n",
    "\n",
    "# LLM with function call \n",
    "azurechatmodel = AzureChatOpenAI(\n",
    "    openai_api_version=os.environ[\"AZURE_OPENAI_API_VERSION\"],\n",
    "    azure_deployment=os.environ[\"AZURE_OPENAI_CHAT_DEPLOYMENT_NAME\"],\n",
    ")\n",
    "structured_llm = azurechatmodel.with_structured_output(RouteQuery)\n",
    "\n",
    "# Prompt \n",
    "system = \"\"\"You are an expert at routing a user question to the appropriate data source.\n",
    "\n",
    "Based on the department the question is referring to, route it to the relevant data source.\"\"\"\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", system),\n",
    "        (\"human\", \"{question}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Define router \n",
    "router = prompt | structured_llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatPromptTemplate(input_variables=['question'], messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=[], template='You are an expert at routing a user question to the appropriate data source.\\n\\nBased on the department the question is referring to, route it to the relevant data source.')), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['question'], template='{question}'))])\n",
       "| RunnableBinding(bound=AzureChatOpenAI(client=<openai.resources.chat.completions.Completions object at 0x0000020F97161C10>, async_client=<openai.resources.chat.completions.AsyncCompletions object at 0x0000020F955E13D0>, openai_api_key=SecretStr('**********'), openai_proxy='', azure_endpoint='https://dskumar.openai.azure.com/', deployment_name='DskumarDeployment', openai_api_version='2023-07-01-preview', openai_api_type='azure'), kwargs={'tools': [{'type': 'function', 'function': {'name': 'RouteQuery', 'description': 'Route a user query to the most relevant datasource.', 'parameters': {'type': 'object', 'properties': {'datasource': {'description': 'Given a user question choose which datasource would be most relevant for answering their question', 'enum': ['tenk_docs', 'Process_docs'], 'type': 'string'}}, 'required': ['datasource']}}}], 'tool_choice': {'type': 'function', 'function': {'name': 'RouteQuery'}}})\n",
       "| PydanticToolsParser(first_tool_only=True, tools=[<class '__main__.RouteQuery'>])"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "router"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"\"\"what is the finacial statement of the company?\"\"\"\n",
    "\n",
    "result = router.invoke({\"question\": question})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RouteQuery(datasource='tenk_docs')"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"\"\"what is the finacial statement of the company?\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages([\"human\", \"speak in {language}\"])\n",
    "prompt.invoke(\"english\")\n",
    "\"\"\"\n",
    "\n",
    "result = router.invoke({\"question\": question})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RouteQuery(datasource='tenk_docs')"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define your retrievers\n",
    "retrievers = {\n",
    "    \"tenk_docs\": \"tenk_retriever\",\n",
    "    \"Process_docs\": \"Process_docs_retriever\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Updated choose_route function\n",
    "def choose_route(result):\n",
    "    if \"10K_docs\" in result.datasource:\n",
    "        return retrievers[\"10K_docs\"]\n",
    "    elif \"process_docs\" in result.datasource:\n",
    "            \n",
    "    else:\n",
    "        return \"No logical docs\"\n",
    "\n",
    "from langchain_core.runnables import RunnableLambda\n",
    "\n",
    "full_chain = router | RunnableLambda(choose_route)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "retriver = full_chain.invoke({\"question\": question})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'tenk_retriever'"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retriver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.tools.retriever import create_retriever_tool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever_tool = create_retriever_tool(name='retriever',description=\"retriever\", retriever=tenk_retriever)\n",
    "tools = [retriever_tool]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Prompt missing required variables: {'agent_scratchpad'}",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[53], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlangchain\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01magents\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m create_tool_calling_agent\n\u001b[1;32m----> 3\u001b[0m agent \u001b[38;5;241m=\u001b[39m \u001b[43mcreate_tool_calling_agent\u001b[49m\u001b[43m(\u001b[49m\u001b[43mazurechatmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtools\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprompt\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprompt\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\langchain\\agents\\tool_calling_agent\\base.py:82\u001b[0m, in \u001b[0;36mcreate_tool_calling_agent\u001b[1;34m(llm, tools, prompt)\u001b[0m\n\u001b[0;32m     78\u001b[0m missing_vars \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124magent_scratchpad\u001b[39m\u001b[38;5;124m\"\u001b[39m}\u001b[38;5;241m.\u001b[39mdifference(\n\u001b[0;32m     79\u001b[0m     prompt\u001b[38;5;241m.\u001b[39minput_variables \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mlist\u001b[39m(prompt\u001b[38;5;241m.\u001b[39mpartial_variables)\n\u001b[0;32m     80\u001b[0m )\n\u001b[0;32m     81\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m missing_vars:\n\u001b[1;32m---> 82\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPrompt missing required variables: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmissing_vars\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     84\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(llm, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbind_tools\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m     85\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m     86\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThis function requires a .bind_tools method be implemented on the LLM.\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m     87\u001b[0m     )\n",
      "\u001b[1;31mValueError\u001b[0m: Prompt missing required variables: {'agent_scratchpad'}"
     ]
    }
   ],
   "source": [
    "from langchain.agents import create_tool_calling_agent\n",
    "\n",
    "agent = create_tool_calling_agent(azurechatmodel, tools, prompt=prompt)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sematic routing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.utils.math import cosine_similarity\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_core.runnables import RunnableLambda, RunnablePassthrough\n",
    "#from langchain_openai import ChatOpenAI, OpenAIEmbeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Two prompts\n",
    "template_10K = \"\"\"You are a very smart at reading finacial statement of a company. \\\n",
    "You are great at answering questions about 10K document in a concise and easy to understand manner. \\\n",
    "When you don't know the answer to a question you admit that you don't know.\n",
    "\n",
    "Here is a question:\n",
    "{query}\"\"\"\n",
    "\n",
    "process_template = \"\"\"You are a very good at process management. You are great at answering process management questions. \\\n",
    "You are so good because you are able to break down hard problems into their component parts, \\\n",
    "answer the component parts, and then put them together to answer the broader question.\n",
    "\n",
    "Here is a question:\n",
    "{query}\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Embed prompts\n",
    "from langchain_openai import AzureOpenAIEmbeddings ##embedding type\n",
    "embeddings = AzureOpenAIEmbeddings(model = os.environ[\"LLM_EMBEDDING_MODEL\"],)\n",
    "prompt_templates = [template_10K, process_template]\n",
    "prompt_embeddings = embeddings.embed_documents(prompt_templates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Route question to prompt \n",
    "def prompt_router(input):\n",
    "    # Embed question\n",
    "    query_embedding = embeddings.embed_query(input[\"query\"])\n",
    "    # Compute similarity\n",
    "    similarity = cosine_similarity([query_embedding], prompt_embeddings)[0]\n",
    "    most_similar = prompt_templates[similarity.argmax()]\n",
    "    # Chosen prompt \n",
    "    print(\"Using template_10K\" if most_similar == template_10K else \"Using process\")\n",
    "    return PromptTemplate.from_template(most_similar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using template_10K\n",
      "Summarizing the annual report involves condensing the key information and highlights of a company's financial performance and operations throughout the year. It typically includes a summary of the company's financial statements, such as the balance sheet, income statement, and cash flow statement, along with any significant events or developments that may have influenced the company's performance. The annual report summary aims to provide a concise overview of the company's financial health, strategic goals, major achievements, and potential risks or challenges. It is often presented in a narrative format, highlighting the most important aspects for investors and stakeholders to understand the company's overall performance and prospects.\n"
     ]
    }
   ],
   "source": [
    "chain = (\n",
    "    {\"query\": RunnablePassthrough()}\n",
    "    | RunnableLambda(prompt_router)\n",
    "    | azurechatmodel\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "print(chain.invoke(\"summaries the annual report?\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
