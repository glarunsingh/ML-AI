{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6d251ced",
   "metadata": {},
   "source": [
    "# How to use LangChain and Azure OpenAI with Python\n",
    "\n",
    "\n",
    "Langchain is an open source framework for developing applications using large language models (LLM). <br>\n",
    "\n",
    "This guide will demonstrate how to setup and use Azure OpenAI models' API with LangChain.\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d0ee335",
   "metadata": {},
   "source": [
    "## Set Up\n",
    "The following libraries must be installed to use LangChain with Azure OpenAI.<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "35289cea",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install --upgrade openai\n",
    "#!pip install langchain"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba880453",
   "metadata": {},
   "source": [
    "## API Configuation and Deployed Model Setup\n",
    "\n",
    "After installing the necessary libraies, the API must be configured. The code below shows how to configure the API directly in your Python environment. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a9752fda",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import openai\n",
    "import json\n",
    "import os\n",
    "from langchain.chat_models import AzureChatOpenAI\n",
    "from langchain.schema import HumanMessage\n",
    "from langchain import LLMChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "80e17163-c233-4452-a7fe-a3ddb69a7dc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load config values\n",
    "with open(r'config.json') as config_file:\n",
    "    config_details = json.load(config_file)\n",
    "\n",
    "# The base URL for your Azure OpenAI resource. e.g. \"https://<your resource name>.openai.azure.com\"\n",
    "openai_api_base=config_details['OPENAI_API_BASE']\n",
    "    \n",
    "# API version e.g. \"2023-07-01-preview\"\n",
    "openai_api_version=config_details['OPENAI_API_VERSION']\n",
    "\n",
    "# The name of your Azure OpenAI deployment chat model. e.g. \"gpt-35-turbo-0613\"\n",
    "deployment_name=config_details['DEPLOYMENT_NAME']\n",
    "\n",
    "# The API key for your Azure OpenAI resource.\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"62855d6dd08945819bf83aee0c104127\"\n",
    "openai_api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "# This is set to `azure`\n",
    "openai_api_type=\"azure\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52da840e",
   "metadata": {},
   "source": [
    "## Deployed Model Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "06faad0b-41cc-44d1-a615-baaed4514fd7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\JANU\\AppData\\Roaming\\Python\\Python311\\site-packages\\langchain_core\\_api\\deprecation.py:119: LangChainDeprecationWarning: The class `AzureChatOpenAI` was deprecated in LangChain 0.0.10 and will be removed in 0.2.0. An updated version of the class exists in the langchain-openai package and should be used instead. To use it run `pip install -U langchain-openai` and import as `from langchain_openai import AzureChatOpenAI`.\n",
      "  warn_deprecated(\n"
     ]
    }
   ],
   "source": [
    "# Create an instance of chat model\n",
    "chat_model = AzureChatOpenAI(\n",
    "    azure_deployment=deployment_name,\n",
    "    openai_api_version=openai_api_version,\n",
    "    azure_endpoint=openai_api_base\n",
    ")\n",
    "\n",
    "response = chat_model.invoke([HumanMessage(content=\"Write me a poem\")])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "daeeb1d8-f74e-47e8-9d21-4f38482e75ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In the realm of dreams, where thoughts take flight,\n",
      "A poetic journey, begins tonight.\n",
      "Where words dance upon the page,\n",
      "A symphony of emotions, like a magical stage.\n",
      "\n",
      "Within the depths of a starlit sky,\n",
      "Words paint the canvas, as time goes by.\n",
      "Whispers of love, like gentle breeze,\n",
      "Caress the heart, put the mind at ease.\n",
      "\n",
      "Through fields of flowers, vibrant and bright,\n",
      "Verse blooms with colors, a pure delight.\n",
      "Where nature's beauty, in every scene,\n",
      "Inspires the poet, to share what's seen.\n",
      "\n",
      "In the depths of sadness, where tears may fall,\n",
      "Poetry becomes a comforting call.\n",
      "It weaves its way through sorrow's embrace,\n",
      "Offering solace in delicate grace.\n",
      "\n",
      "Through the passage of time, the poem unfolds,\n",
      "Like a story written, in secrets untold.\n",
      "It captures moments, frozen in rhyme,\n",
      "Preserving memories, for future time.\n",
      "\n",
      "With each line penned, emotions unfurl,\n",
      "Like waves crashing upon the shore's pearl.\n",
      "Poetry, the language of the soul,\n",
      "Expresses the unspoken, makes us whole.\n",
      "\n",
      "So let us embrace this lyrical art,\n",
      "And let our words ignite the heart.\n",
      "For in the realm of poetry, we find,\n",
      "A refuge for the soul, a peace of mind.\n"
     ]
    }
   ],
   "source": [
    "print(response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95ec9997-bbd3-44ae-945c-5f25c33dfd96",
   "metadata": {},
   "source": [
    "# Azure LLM "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6d84e71b-fb59-49e7-90c2-51bcd9db262b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now lets create a chain with the normal OpenAI model\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "#from langchain_openai import OpenAI\n",
    "# Import Azure OpenAI\n",
    "from langchain_openai import AzureOpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "cb9e3676-77b3-4a1a-8a33-dcae3623f7ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = AzureOpenAI(\n",
    "    azure_deployment=deployment_name,\n",
    "    openai_api_version=openai_api_version,\n",
    "    azure_endpoint=openai_api_base\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e56360b8-c37a-4256-8c96-ebc6b3b7acfc",
   "metadata": {},
   "outputs": [
    {
     "ename": "BadRequestError",
     "evalue": "Error code: 400 - {'error': {'code': 'OperationNotSupported', 'message': 'The completion operation does not work with the specified model, gpt-35-turbo-16k. Please choose different model and try again. You can learn more about which models can be used with each operation here: https://go.microsoft.com/fwlink/?linkid=2197993.'}}",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mBadRequestError\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[23], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m llm\u001b[38;5;241m.\u001b[39minvoke(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTell me about yourself\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\langchain_core\\language_models\\llms.py:276\u001b[0m, in \u001b[0;36mBaseLLM.invoke\u001b[1;34m(self, input, config, stop, **kwargs)\u001b[0m\n\u001b[0;32m    266\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21minvoke\u001b[39m(\n\u001b[0;32m    267\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    268\u001b[0m     \u001b[38;5;28minput\u001b[39m: LanguageModelInput,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    272\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[0;32m    273\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mstr\u001b[39m:\n\u001b[0;32m    274\u001b[0m     config \u001b[38;5;241m=\u001b[39m ensure_config(config)\n\u001b[0;32m    275\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m (\n\u001b[1;32m--> 276\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgenerate_prompt(\n\u001b[0;32m    277\u001b[0m             [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_convert_input(\u001b[38;5;28minput\u001b[39m)],\n\u001b[0;32m    278\u001b[0m             stop\u001b[38;5;241m=\u001b[39mstop,\n\u001b[0;32m    279\u001b[0m             callbacks\u001b[38;5;241m=\u001b[39mconfig\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcallbacks\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[0;32m    280\u001b[0m             tags\u001b[38;5;241m=\u001b[39mconfig\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtags\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[0;32m    281\u001b[0m             metadata\u001b[38;5;241m=\u001b[39mconfig\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmetadata\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[0;32m    282\u001b[0m             run_name\u001b[38;5;241m=\u001b[39mconfig\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrun_name\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[0;32m    283\u001b[0m             run_id\u001b[38;5;241m=\u001b[39mconfig\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrun_id\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[0;32m    284\u001b[0m             \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m    285\u001b[0m         )\n\u001b[0;32m    286\u001b[0m         \u001b[38;5;241m.\u001b[39mgenerations[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m    287\u001b[0m         \u001b[38;5;241m.\u001b[39mtext\n\u001b[0;32m    288\u001b[0m     )\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\langchain_core\\language_models\\llms.py:633\u001b[0m, in \u001b[0;36mBaseLLM.generate_prompt\u001b[1;34m(self, prompts, stop, callbacks, **kwargs)\u001b[0m\n\u001b[0;32m    625\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mgenerate_prompt\u001b[39m(\n\u001b[0;32m    626\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    627\u001b[0m     prompts: List[PromptValue],\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    630\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[0;32m    631\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m LLMResult:\n\u001b[0;32m    632\u001b[0m     prompt_strings \u001b[38;5;241m=\u001b[39m [p\u001b[38;5;241m.\u001b[39mto_string() \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m prompts]\n\u001b[1;32m--> 633\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgenerate(prompt_strings, stop\u001b[38;5;241m=\u001b[39mstop, callbacks\u001b[38;5;241m=\u001b[39mcallbacks, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\langchain_core\\language_models\\llms.py:803\u001b[0m, in \u001b[0;36mBaseLLM.generate\u001b[1;34m(self, prompts, stop, callbacks, tags, metadata, run_name, run_id, **kwargs)\u001b[0m\n\u001b[0;32m    788\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcache \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m get_llm_cache() \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcache \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m:\n\u001b[0;32m    789\u001b[0m     run_managers \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m    790\u001b[0m         callback_manager\u001b[38;5;241m.\u001b[39mon_llm_start(\n\u001b[0;32m    791\u001b[0m             dumpd(\u001b[38;5;28mself\u001b[39m),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    801\u001b[0m         )\n\u001b[0;32m    802\u001b[0m     ]\n\u001b[1;32m--> 803\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_generate_helper(\n\u001b[0;32m    804\u001b[0m         prompts, stop, run_managers, \u001b[38;5;28mbool\u001b[39m(new_arg_supported), \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[0;32m    805\u001b[0m     )\n\u001b[0;32m    806\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m output\n\u001b[0;32m    807\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(missing_prompts) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\langchain_core\\language_models\\llms.py:670\u001b[0m, in \u001b[0;36mBaseLLM._generate_helper\u001b[1;34m(self, prompts, stop, run_managers, new_arg_supported, **kwargs)\u001b[0m\n\u001b[0;32m    668\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m run_manager \u001b[38;5;129;01min\u001b[39;00m run_managers:\n\u001b[0;32m    669\u001b[0m         run_manager\u001b[38;5;241m.\u001b[39mon_llm_error(e, response\u001b[38;5;241m=\u001b[39mLLMResult(generations\u001b[38;5;241m=\u001b[39m[]))\n\u001b[1;32m--> 670\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[0;32m    671\u001b[0m flattened_outputs \u001b[38;5;241m=\u001b[39m output\u001b[38;5;241m.\u001b[39mflatten()\n\u001b[0;32m    672\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m manager, flattened_output \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(run_managers, flattened_outputs):\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\langchain_core\\language_models\\llms.py:657\u001b[0m, in \u001b[0;36mBaseLLM._generate_helper\u001b[1;34m(self, prompts, stop, run_managers, new_arg_supported, **kwargs)\u001b[0m\n\u001b[0;32m    647\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_generate_helper\u001b[39m(\n\u001b[0;32m    648\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    649\u001b[0m     prompts: List[\u001b[38;5;28mstr\u001b[39m],\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    653\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[0;32m    654\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m LLMResult:\n\u001b[0;32m    655\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    656\u001b[0m         output \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m--> 657\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_generate(\n\u001b[0;32m    658\u001b[0m                 prompts,\n\u001b[0;32m    659\u001b[0m                 stop\u001b[38;5;241m=\u001b[39mstop,\n\u001b[0;32m    660\u001b[0m                 \u001b[38;5;66;03m# TODO: support multiple run managers\u001b[39;00m\n\u001b[0;32m    661\u001b[0m                 run_manager\u001b[38;5;241m=\u001b[39mrun_managers[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m run_managers \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    662\u001b[0m                 \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m    663\u001b[0m             )\n\u001b[0;32m    664\u001b[0m             \u001b[38;5;28;01mif\u001b[39;00m new_arg_supported\n\u001b[0;32m    665\u001b[0m             \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_generate(prompts, stop\u001b[38;5;241m=\u001b[39mstop)\n\u001b[0;32m    666\u001b[0m         )\n\u001b[0;32m    667\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    668\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m run_manager \u001b[38;5;129;01min\u001b[39;00m run_managers:\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\langchain_openai\\llms\\base.py:368\u001b[0m, in \u001b[0;36mBaseOpenAI._generate\u001b[1;34m(self, prompts, stop, run_manager, **kwargs)\u001b[0m\n\u001b[0;32m    352\u001b[0m     choices\u001b[38;5;241m.\u001b[39mappend(\n\u001b[0;32m    353\u001b[0m         {\n\u001b[0;32m    354\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m\"\u001b[39m: generation\u001b[38;5;241m.\u001b[39mtext,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    365\u001b[0m         }\n\u001b[0;32m    366\u001b[0m     )\n\u001b[0;32m    367\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 368\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclient\u001b[38;5;241m.\u001b[39mcreate(prompt\u001b[38;5;241m=\u001b[39m_prompts, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mparams)\n\u001b[0;32m    369\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(response, \u001b[38;5;28mdict\u001b[39m):\n\u001b[0;32m    370\u001b[0m         \u001b[38;5;66;03m# V1 client returns the response in an PyDantic object instead of\u001b[39;00m\n\u001b[0;32m    371\u001b[0m         \u001b[38;5;66;03m# dict. For the transition period, we deep convert it to dict.\u001b[39;00m\n\u001b[0;32m    372\u001b[0m         response \u001b[38;5;241m=\u001b[39m response\u001b[38;5;241m.\u001b[39mmodel_dump()\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\openai\\_utils\\_utils.py:275\u001b[0m, in \u001b[0;36mrequired_args.<locals>.inner.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    273\u001b[0m             msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMissing required argument: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mquote(missing[\u001b[38;5;241m0\u001b[39m])\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    274\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(msg)\n\u001b[1;32m--> 275\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\openai\\resources\\completions.py:516\u001b[0m, in \u001b[0;36mCompletions.create\u001b[1;34m(self, model, prompt, best_of, echo, frequency_penalty, logit_bias, logprobs, max_tokens, n, presence_penalty, seed, stop, stream, suffix, temperature, top_p, user, extra_headers, extra_query, extra_body, timeout)\u001b[0m\n\u001b[0;32m    488\u001b[0m \u001b[38;5;129m@required_args\u001b[39m([\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mprompt\u001b[39m\u001b[38;5;124m\"\u001b[39m], [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mprompt\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstream\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[0;32m    489\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcreate\u001b[39m(\n\u001b[0;32m    490\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    514\u001b[0m     timeout: \u001b[38;5;28mfloat\u001b[39m \u001b[38;5;241m|\u001b[39m httpx\u001b[38;5;241m.\u001b[39mTimeout \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m|\u001b[39m NotGiven \u001b[38;5;241m=\u001b[39m NOT_GIVEN,\n\u001b[0;32m    515\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Completion \u001b[38;5;241m|\u001b[39m Stream[Completion]:\n\u001b[1;32m--> 516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_post(\n\u001b[0;32m    517\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/completions\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    518\u001b[0m         body\u001b[38;5;241m=\u001b[39mmaybe_transform(\n\u001b[0;32m    519\u001b[0m             {\n\u001b[0;32m    520\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m\"\u001b[39m: model,\n\u001b[0;32m    521\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mprompt\u001b[39m\u001b[38;5;124m\"\u001b[39m: prompt,\n\u001b[0;32m    522\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbest_of\u001b[39m\u001b[38;5;124m\"\u001b[39m: best_of,\n\u001b[0;32m    523\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mecho\u001b[39m\u001b[38;5;124m\"\u001b[39m: echo,\n\u001b[0;32m    524\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfrequency_penalty\u001b[39m\u001b[38;5;124m\"\u001b[39m: frequency_penalty,\n\u001b[0;32m    525\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlogit_bias\u001b[39m\u001b[38;5;124m\"\u001b[39m: logit_bias,\n\u001b[0;32m    526\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlogprobs\u001b[39m\u001b[38;5;124m\"\u001b[39m: logprobs,\n\u001b[0;32m    527\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmax_tokens\u001b[39m\u001b[38;5;124m\"\u001b[39m: max_tokens,\n\u001b[0;32m    528\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mn\u001b[39m\u001b[38;5;124m\"\u001b[39m: n,\n\u001b[0;32m    529\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpresence_penalty\u001b[39m\u001b[38;5;124m\"\u001b[39m: presence_penalty,\n\u001b[0;32m    530\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mseed\u001b[39m\u001b[38;5;124m\"\u001b[39m: seed,\n\u001b[0;32m    531\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstop\u001b[39m\u001b[38;5;124m\"\u001b[39m: stop,\n\u001b[0;32m    532\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstream\u001b[39m\u001b[38;5;124m\"\u001b[39m: stream,\n\u001b[0;32m    533\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msuffix\u001b[39m\u001b[38;5;124m\"\u001b[39m: suffix,\n\u001b[0;32m    534\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtemperature\u001b[39m\u001b[38;5;124m\"\u001b[39m: temperature,\n\u001b[0;32m    535\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtop_p\u001b[39m\u001b[38;5;124m\"\u001b[39m: top_p,\n\u001b[0;32m    536\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muser\u001b[39m\u001b[38;5;124m\"\u001b[39m: user,\n\u001b[0;32m    537\u001b[0m             },\n\u001b[0;32m    538\u001b[0m             completion_create_params\u001b[38;5;241m.\u001b[39mCompletionCreateParams,\n\u001b[0;32m    539\u001b[0m         ),\n\u001b[0;32m    540\u001b[0m         options\u001b[38;5;241m=\u001b[39mmake_request_options(\n\u001b[0;32m    541\u001b[0m             extra_headers\u001b[38;5;241m=\u001b[39mextra_headers, extra_query\u001b[38;5;241m=\u001b[39mextra_query, extra_body\u001b[38;5;241m=\u001b[39mextra_body, timeout\u001b[38;5;241m=\u001b[39mtimeout\n\u001b[0;32m    542\u001b[0m         ),\n\u001b[0;32m    543\u001b[0m         cast_to\u001b[38;5;241m=\u001b[39mCompletion,\n\u001b[0;32m    544\u001b[0m         stream\u001b[38;5;241m=\u001b[39mstream \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m    545\u001b[0m         stream_cls\u001b[38;5;241m=\u001b[39mStream[Completion],\n\u001b[0;32m    546\u001b[0m     )\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\openai\\_base_client.py:1233\u001b[0m, in \u001b[0;36mSyncAPIClient.post\u001b[1;34m(self, path, cast_to, body, options, files, stream, stream_cls)\u001b[0m\n\u001b[0;32m   1219\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpost\u001b[39m(\n\u001b[0;32m   1220\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   1221\u001b[0m     path: \u001b[38;5;28mstr\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1228\u001b[0m     stream_cls: \u001b[38;5;28mtype\u001b[39m[_StreamT] \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m   1229\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ResponseT \u001b[38;5;241m|\u001b[39m _StreamT:\n\u001b[0;32m   1230\u001b[0m     opts \u001b[38;5;241m=\u001b[39m FinalRequestOptions\u001b[38;5;241m.\u001b[39mconstruct(\n\u001b[0;32m   1231\u001b[0m         method\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpost\u001b[39m\u001b[38;5;124m\"\u001b[39m, url\u001b[38;5;241m=\u001b[39mpath, json_data\u001b[38;5;241m=\u001b[39mbody, files\u001b[38;5;241m=\u001b[39mto_httpx_files(files), \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39moptions\n\u001b[0;32m   1232\u001b[0m     )\n\u001b[1;32m-> 1233\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m cast(ResponseT, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrequest(cast_to, opts, stream\u001b[38;5;241m=\u001b[39mstream, stream_cls\u001b[38;5;241m=\u001b[39mstream_cls))\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\openai\\_base_client.py:922\u001b[0m, in \u001b[0;36mSyncAPIClient.request\u001b[1;34m(self, cast_to, options, remaining_retries, stream, stream_cls)\u001b[0m\n\u001b[0;32m    913\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrequest\u001b[39m(\n\u001b[0;32m    914\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    915\u001b[0m     cast_to: Type[ResponseT],\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    920\u001b[0m     stream_cls: \u001b[38;5;28mtype\u001b[39m[_StreamT] \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    921\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ResponseT \u001b[38;5;241m|\u001b[39m _StreamT:\n\u001b[1;32m--> 922\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_request(\n\u001b[0;32m    923\u001b[0m         cast_to\u001b[38;5;241m=\u001b[39mcast_to,\n\u001b[0;32m    924\u001b[0m         options\u001b[38;5;241m=\u001b[39moptions,\n\u001b[0;32m    925\u001b[0m         stream\u001b[38;5;241m=\u001b[39mstream,\n\u001b[0;32m    926\u001b[0m         stream_cls\u001b[38;5;241m=\u001b[39mstream_cls,\n\u001b[0;32m    927\u001b[0m         remaining_retries\u001b[38;5;241m=\u001b[39mremaining_retries,\n\u001b[0;32m    928\u001b[0m     )\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\openai\\_base_client.py:1013\u001b[0m, in \u001b[0;36mSyncAPIClient._request\u001b[1;34m(self, cast_to, options, remaining_retries, stream, stream_cls)\u001b[0m\n\u001b[0;32m   1010\u001b[0m         err\u001b[38;5;241m.\u001b[39mresponse\u001b[38;5;241m.\u001b[39mread()\n\u001b[0;32m   1012\u001b[0m     log\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRe-raising status error\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m-> 1013\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_status_error_from_response(err\u001b[38;5;241m.\u001b[39mresponse) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1015\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_process_response(\n\u001b[0;32m   1016\u001b[0m     cast_to\u001b[38;5;241m=\u001b[39mcast_to,\n\u001b[0;32m   1017\u001b[0m     options\u001b[38;5;241m=\u001b[39moptions,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1020\u001b[0m     stream_cls\u001b[38;5;241m=\u001b[39mstream_cls,\n\u001b[0;32m   1021\u001b[0m )\n",
      "\u001b[1;31mBadRequestError\u001b[0m: Error code: 400 - {'error': {'code': 'OperationNotSupported', 'message': 'The completion operation does not work with the specified model, gpt-35-turbo-16k. Please choose different model and try again. You can learn more about which models can be used with each operation here: https://go.microsoft.com/fwlink/?linkid=2197993.'}}"
     ]
    }
   ],
   "source": [
    "llm.invoke(\"Tell me about yourself\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc7ea2d4",
   "metadata": {},
   "source": [
    "## PromptTemplates\n",
    "\n",
    "Langchain provides a built in PromptsTemplate module to simplify the construction of prompts to get more specific answers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "927d4bac",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain import PromptTemplate\n",
    "\n",
    "from langchain.prompts.chat import (\n",
    "    ChatPromptTemplate,\n",
    "    SystemMessagePromptTemplate,\n",
    "    HumanMessagePromptTemplate,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2308181c-6eec-41ec-a0c8-e7930f27bcde",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First Example\n",
    "template = \"\"\"\n",
    "You are a skin care consulant that recommends products based on customer\n",
    "needs and preferences.\n",
    "\n",
    "What is a good {product_type} to help with {customer_request}?\n",
    "\"\"\"\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    input_variables=[\"product_type\", \"customer_request\"],\n",
    "    template=template,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "8958f8a3-990a-40a0-8dc5-3985ee465dcd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example #1:\n",
      "A good face wash for acne-prone skin is one that effectively cleanses the skin, removes excess oil, unclogs pores, and helps to prevent future breakouts. Here are a few recommendations:\n",
      "\n",
      "1. Neutrogena Oil-Free Acne Wash: This face wash contains salicylic acid, which helps to exfoliate the skin and clear acne. It also helps in reducing inflammation and redness caused by acne.\n",
      "\n",
      "2. La Roche-Posay Effaclar Medicated Gel Cleanser: This gel cleanser is specifically formulated for oily and acne-prone skin. It contains salicylic acid and LHA, which help to unclog pores and reduce excess oil production.\n",
      "\n",
      "3. CeraVe Acne Foaming Cream Cleanser: This cleanser contains benzoyl peroxide, which is effective in treating acne by killing the bacteria that causes breakouts. It also helps in reducing excess oil and improving skin texture.\n",
      "\n",
      "4. Mario Badescu Acne Facial Cleanser: This face wash contains salicylic acid and aloe vera, which work together to reduce acne and soothe the skin. It also helps to unclog pores and prevent future breakouts.\n",
      "\n",
      "5. Bioré Charcoal Acne Clearing Cleanser: This cleanser contains charcoal, which helps to draw out impurities and excess oil from the skin. It also contains salicylic acid to treat and prevent acne.\n",
      "\n",
      "Remember, it's important to choose a face wash that suits your skin type and doesn't cause any irritation. If you have severe acne or any specific concerns, it's best to consult with a dermatologist for personalized recommendations.\n"
     ]
    }
   ],
   "source": [
    "print(\"Example #1:\")\n",
    "response = llm.invoke([HumanMessage(content=prompt.format(\n",
    "        product_type=\"face wash\",\n",
    "        customer_request = \"acne prone skin\"\n",
    "    ))]\n",
    ")\n",
    "\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "502fd3bf-4db5-4016-8c5e-7522e3cf6602",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Second Example\n",
    "system_message = \"You are an AI travel assistant that provides vacation recommendations.\"\n",
    "system_message_prompt = SystemMessagePromptTemplate.from_template(system_message)\n",
    "\n",
    "human_template=\"{text}\"\n",
    "human_message_prompt = HumanMessagePromptTemplate.from_template(human_template)\n",
    "\n",
    "chat_prompt = ChatPromptTemplate.from_messages([system_message_prompt, human_message_prompt])\n",
    "\n",
    "chain = LLMChain(llm=llm, prompt=chat_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "7b1d285b-182c-4e12-a20d-f2e6fcce52f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example #2:\n",
      "For warm weather and beautiful beaches in December, I would recommend the following destinations:\n",
      "\n",
      "1. Maldives: This tropical paradise offers pristine white-sand beaches, crystal-clear turquoise waters, and luxurious resorts. The Maldives experiences year-round warm temperatures, making it an ideal destination for beach lovers.\n",
      "\n",
      "2. Hawaii, USA: With its stunning beaches, lush landscapes, and diverse marine life, Hawaii is a fantastic choice. The islands of Oahu, Maui, and Kauai offer a mix of relaxation and adventure, along with plenty of warm, sunny days.\n",
      "\n",
      "3. Thailand: Head to the southern islands of Thailand, such as Phuket, Krabi, or Koh Samui, for a beach getaway. These destinations boast stunning beaches, warm waters, and a vibrant local culture. December is also a great time to explore Thailand's stunning national parks and enjoy water activities like snorkeling and diving.\n",
      "\n",
      "4. Costa Rica: With its beautiful coastline on both the Pacific Ocean and the Caribbean Sea, Costa Rica offers a variety of beaches to choose from. Enjoy the warm weather, surf the waves, explore national parks, and indulge in adventure activities like zip-lining and hiking.\n",
      "\n",
      "5. Bali, Indonesia: Bali's tropical climate makes it an excellent destination for warm weather in December. The island is famous for its stunning beaches, lush rice terraces, vibrant culture, and delicious cuisine. Visit popular beach towns like Seminyak, Kuta, or Nusa Dua for a relaxing beach vacation.\n",
      "\n",
      "Remember to check travel restrictions and safety guidelines before planning your trip, as they may vary due to the current global situation.\n"
     ]
    }
   ],
   "source": [
    "result = chain.run(f\"Where should I go on vaction in Decemember for warm weather and beaches?\")\n",
    "print(\"Example #2:\")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b6723eb",
   "metadata": {},
   "source": [
    "## Chains\n",
    "There are many applications of chains that allow you to combine numerous LLM calls and actions.  <br>\n",
    "\n",
    "### Simple Sequential Chains  <br>\n",
    "Allow you to feed the output of one LLM Chain as input for another."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "af7c236f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import SimpleSequentialChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "2a4a32f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "description_template = \"\"\"Your job is to come up with a fun DIY project for the specified gender, age, and description of a kid.\n",
    "% CHILD_DESCRIPTION\n",
    "{child_description}\n",
    "\n",
    "YOUR RESPONSE:\n",
    "\"\"\"\n",
    "description_prompt_template = PromptTemplate(input_variables=[\"child_description\"], template=description_template)\n",
    "\n",
    "topic_chain = LLMChain(llm=llm, prompt=description_prompt_template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "594007cd-a609-4c8d-ac5b-8ab312169dbd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LLMChain(prompt=PromptTemplate(input_variables=['child_description'], template='Your job is to come up with a fun DIY project for the specified gender, age, and description of a kid.\\n% CHILD_DESCRIPTION\\n{child_description}\\n\\nYOUR RESPONSE:\\n'), llm=AzureChatOpenAI(client=<openai.resources.chat.completions.Completions object at 0x000001D1E1A0B8D0>, async_client=<openai.resources.chat.completions.AsyncCompletions object at 0x000001D1E1A74110>, openai_api_key='62855d6dd08945819bf83aee0c104127', openai_proxy='', azure_endpoint='https://dskumar.openai.azure.com/', deployment_name='DskumarDeployment', openai_api_version='2023-07-01-preview', openai_api_type='azure'))"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "topic_chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "6eec47ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "diy_description_template = \"\"\"Given a DIY project, give a short and simple recipe step-by-step guide on how to complete the project and a materials list.\n",
    "% DIY_PROJECT\n",
    "{diy_project}\n",
    "\n",
    "YOUR RESPONSE:\n",
    "\"\"\"\n",
    "diy_prompt_template = PromptTemplate(input_variables=[\"diy_project\"], template=diy_description_template)\n",
    "\n",
    "description_chain = LLMChain(llm=llm, prompt=diy_prompt_template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "f98c1f58-8b12-49a0-b78f-81cf31eac5b0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LLMChain(prompt=PromptTemplate(input_variables=['diy_project'], template='Given a DIY project, give a short and simple recipe step-by-step guide on how to complete the project and a materials list.\\n% DIY_PROJECT\\n{diy_project}\\n\\nYOUR RESPONSE:\\n'), llm=AzureChatOpenAI(client=<openai.resources.chat.completions.Completions object at 0x000001D1E1A0B8D0>, async_client=<openai.resources.chat.completions.AsyncCompletions object at 0x000001D1E1A74110>, openai_api_key='62855d6dd08945819bf83aee0c104127', openai_proxy='', azure_endpoint='https://dskumar.openai.azure.com/', deployment_name='DskumarDeployment', openai_api_version='2023-07-01-preview', openai_api_type='azure'))"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "description_chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "84a15aea",
   "metadata": {},
   "outputs": [],
   "source": [
    "overall_chain = SimpleSequentialChain(chains=[topic_chain, description_chain, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "57a950bd-f035-41b2-a776-461274b82429",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SimpleSequentialChain(verbose=True, chains=[LLMChain(prompt=PromptTemplate(input_variables=['child_description'], template='Your job is to come up with a fun DIY project for the specified gender, age, and description of a kid.\\n% CHILD_DESCRIPTION\\n{child_description}\\n\\nYOUR RESPONSE:\\n'), llm=AzureChatOpenAI(client=<openai.resources.chat.completions.Completions object at 0x000001D1E1A0B8D0>, async_client=<openai.resources.chat.completions.AsyncCompletions object at 0x000001D1E1A74110>, openai_api_key='62855d6dd08945819bf83aee0c104127', openai_proxy='', azure_endpoint='https://dskumar.openai.azure.com/', deployment_name='DskumarDeployment', openai_api_version='2023-07-01-preview', openai_api_type='azure')), LLMChain(prompt=PromptTemplate(input_variables=['diy_project'], template='Given a DIY project, give a short and simple recipe step-by-step guide on how to complete the project and a materials list.\\n% DIY_PROJECT\\n{diy_project}\\n\\nYOUR RESPONSE:\\n'), llm=AzureChatOpenAI(client=<openai.resources.chat.completions.Completions object at 0x000001D1E1A0B8D0>, async_client=<openai.resources.chat.completions.AsyncCompletions object at 0x000001D1E1A74110>, openai_api_key='62855d6dd08945819bf83aee0c104127', openai_proxy='', azure_endpoint='https://dskumar.openai.azure.com/', deployment_name='DskumarDeployment', openai_api_version='2023-07-01-preview', openai_api_type='azure'))])"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "overall_chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "15928f72",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new SimpleSequentialChain chain...\u001b[0m\n",
      "\u001b[36;1m\u001b[1;3mDIY Project: Build Your Own Miniature Skate Park\n",
      "\n",
      "Description:\n",
      "For the adventurous and creative 10-year-old boy, a fun DIY project would be to build his very own miniature skate park! This project will allow him to unleash his imagination and create a customized skate park that he can use with his skateboards, fingerboards, or toy cars.\n",
      "\n",
      "Materials Needed:\n",
      "1. Cardboard or foam board\n",
      "2. Craft supplies (markers, colored paper, stickers, etc.)\n",
      "3. Scissors\n",
      "4. Hot glue gun or craft glue\n",
      "5. Small wooden blocks or building blocks\n",
      "6. Small ramps or inclines (can be made from cardboard or foam)\n",
      "7. Optional: Toy skateboards, fingerboards, or toy cars\n",
      "\n",
      "Instructions:\n",
      "1. Start by creating a base for the skate park using a large piece of cardboard or foam board. This will be the foundation on which the ramps and obstacles will be placed.\n",
      "\n",
      "2. Use the scissors to cut out smaller pieces of cardboard or foam board to create ramps, half-pipes, and other skate park features. Be creative with the shapes and sizes to make the skate park more exciting.\n",
      "\n",
      "3. Decorate the ramps and obstacles using markers, colored paper, stickers, or any other craft supplies. Add designs, graffiti, or logos to give the skate park a personalized touch.\n",
      "\n",
      "4. Use the hot glue gun or craft glue to secure the ramps and obstacles onto the base. Make sure they are stable and won't move around while playing with the skate park.\n",
      "\n",
      "5. If you have small wooden blocks or building blocks, you can use them to create additional structures like benches, stairs, or rails. Glue them onto the base or attach them to the ramps to create more challenging obstacles.\n",
      "\n",
      "6. Add any additional details or accessories you'd like, such as small toy skateboards, fingerboards, or toy cars to bring the skate park to life.\n",
      "\n",
      "7. Once everything is securely in place, let your creativity run wild and enjoy using your new DIY miniature skate park! You can test out different tricks, set up challenges, or invite friends over for some friendly competition.\n",
      "\n",
      "Remember, safety is essential, so make sure to wear protective gear like helmets and knee pads while using the skate park. Have fun and keep exploring new ways to enhance your skate park with additional features or modifications!\u001b[0m\n",
      "\u001b[33;1m\u001b[1;3mMaterials Needed:\n",
      "1. Cardboard or foam board\n",
      "2. Craft supplies (markers, colored paper, stickers, etc.)\n",
      "3. Scissors\n",
      "4. Hot glue gun or craft glue\n",
      "5. Small wooden blocks or building blocks\n",
      "6. Small ramps or inclines (can be made from cardboard or foam)\n",
      "7. Optional: Toy skateboards, fingerboards, or toy cars\n",
      "\n",
      "Instructions:\n",
      "1. Create a base for the skate park using a large piece of cardboard or foam board.\n",
      "2. Cut smaller pieces of cardboard or foam board to create ramps, half-pipes, and other skate park features.\n",
      "3. Decorate the ramps and obstacles using markers, colored paper, stickers, or other craft supplies.\n",
      "4. Secure the ramps and obstacles onto the base using a hot glue gun or craft glue.\n",
      "5. Use small wooden blocks or building blocks to create additional structures like benches, stairs, or rails.\n",
      "6. Glue the additional structures onto the base or attach them to the ramps for more challenging obstacles.\n",
      "7. Add any additional details or accessories, such as toy skateboards, fingerboards, or toy cars.\n",
      "8. Ensure everything is securely in place and enjoy using your DIY miniature skate park.\n",
      "9. Remember to wear protective gear like helmets and knee pads while using the skate park for safety.\n",
      "10. Have fun and continue to explore ways to enhance your skate park with additional features or modifications.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "review = overall_chain.run(\"10-year-old boy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "513e247c-06d4-4f29-a93e-894c2cf471a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Materials Needed:\n",
      "1. Cardboard or foam board\n",
      "2. Craft supplies (markers, colored paper, stickers, etc.)\n",
      "3. Scissors\n",
      "4. Hot glue gun or craft glue\n",
      "5. Small wooden blocks or building blocks\n",
      "6. Small ramps or inclines (can be made from cardboard or foam)\n",
      "7. Optional: Toy skateboards, fingerboards, or toy cars\n",
      "\n",
      "Instructions:\n",
      "1. Create a base for the skate park using a large piece of cardboard or foam board.\n",
      "2. Cut smaller pieces of cardboard or foam board to create ramps, half-pipes, and other skate park features.\n",
      "3. Decorate the ramps and obstacles using markers, colored paper, stickers, or other craft supplies.\n",
      "4. Secure the ramps and obstacles onto the base using a hot glue gun or craft glue.\n",
      "5. Use small wooden blocks or building blocks to create additional structures like benches, stairs, or rails.\n",
      "6. Glue the additional structures onto the base or attach them to the ramps for more challenging obstacles.\n",
      "7. Add any additional details or accessories, such as toy skateboards, fingerboards, or toy cars.\n",
      "8. Ensure everything is securely in place and enjoy using your DIY miniature skate park.\n",
      "9. Remember to wear protective gear like helmets and knee pads while using the skate park for safety.\n",
      "10. Have fun and continue to explore ways to enhance your skate park with additional features or modifications.\n"
     ]
    }
   ],
   "source": [
    "print(review)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "126e87e3-6379-4622-82e0-a390e13db8c1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
